# -*- coding: utf-8 -*-
"""NLP - David William Tanto, Oei.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_tBjqz8Hw8mQh_1EhhEvV6x3k1MSiJcj

# Data Diri
Nama : David William Tanto, Oei

# 1. Import Module
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences
# %matplotlib inline

"""# 2. Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# 3. Import Dataset"""

data = pd.read_csv('/content/drive/MyDrive/Indosat/kursus4/NLP/spam.csv', encoding="ISO-8859-1")

data.head(5)

data.tail(5)

data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)
data.info()

print("Dataset size:", len(data))

"""## Description Data

Understand the distribution better
"""

sns.countplot(data.v1)
plt.xlabel('Label')
plt.title('Number of ham and spam messages')

"""# 3. Preprocessing

## Label Encoder

1.   Create input and output vectors.
2.   Process the labels.
"""

X = data.v2
Y = data.v1
le = LabelEncoder()
Y = le.fit_transform(Y)
Y = Y.reshape(-1,1)

"""# 4. Training Data

Split into training and test data.
"""

X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size = 0.20)

"""**Process the data**
*   Tokenize the data and convert the text to sequences.
*   Add padding to ensure that all the sequences have the same shape.
*   There are many ways of taking the max_len and here an arbitrary length of 150 is chosen.



"""

max_words = 1000
max_len = 150
tok = Tokenizer(num_words = max_words)
tok.fit_on_texts(X_train)
sequences = tok.texts_to_sequences(X_train)
sequences_matrix = pad_sequences(sequences, maxlen = max_len)

"""## LSTM Model"""

from keras.models import Sequential

def RNN():
    inputs = Input(name='inputs',shape = [max_len])
    layer = Embedding(max_words, 50, input_length = max_len)(inputs)
    layer = LSTM(200)(layer)
    layer = Dense(256, name = 'FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.2)(layer)
    layer = Dense(1, name = 'out_layer')(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs,outputs=layer)
    return model

model = RNN()
model.summary()
model.compile(loss = 'binary_crossentropy',
              optimizer = RMSprop(),
              metrics = ['accuracy'])

"""## Visualisasi Model"""

import tensorflow as tf

tf.keras.utils.plot_model(model, 
                          show_shapes = True, 
                          show_dtype = False, 
                          show_layer_names = True, 
                          expand_nested = True, 
                          show_layer_activations = True)

hist = model.fit(sequences_matrix, Y_train,
                 batch_size = 200,
                 epochs = 10,
                 validation_split = 0.2,
                 callbacks = [EarlyStopping(monitor = 'val_loss',
                                            min_delta = 0.0001)])

test_sequences = tok.texts_to_sequences(X_test)
test_sequences_matrix = pad_sequences(test_sequences, maxlen = max_len)

"""## Evaluate Model"""

score_training = model.evaluate(sequences_matrix, Y_train, verbose = 1)
print('Training loss :', score_training)

score_testing = model.evaluate(test_sequences_matrix,Y_test)
print('Test loss :', score_testing)

"""## Visualisasi Training Model Loss"""

plt.plot(hist.history['loss'])
plt.title('Training model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()